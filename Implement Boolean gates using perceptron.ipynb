{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34nKSMt5sraz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.01, epochs=1000, weights=None):\n",
        "        # Initialize weights and bias\n",
        "        if weights is None:\n",
        "            self.weights = np.zeros(input_size + 1)  # +1 for bias\n",
        "        else:\n",
        "            self.weights = np.array(weights)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(x, self.weights[1:]) + self.weights[0]\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            for xi, yi in zip(X, y):\n",
        "                prediction = self.predict(xi)\n",
        "                self.weights[1:] += self.learning_rate * (yi - prediction) * xi\n",
        "                self.weights[0] += self.learning_rate * (yi - prediction)\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_input():\n",
        "    print(\"Define your dataset for training the perceptron.\")\n",
        "\n",
        "    # Get the number of features\n",
        "    input_size = int(input(\"Enter the number of features (e.g., 2 for x1, x2): \"))\n",
        "\n",
        "    # Get the number of data points\n",
        "    num_samples = int(input(\"Enter the number of data points: \"))\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Get data points from the user\n",
        "    print(f\"Enter {num_samples} data points, each with {input_size} features (e.g., '0 1' for 2 features):\")\n",
        "    for _ in range(num_samples):\n",
        "        features = list(map(float, input().split()))\n",
        "        X.append(features)\n",
        "\n",
        "    # Get labels from the user\n",
        "    print(\"Enter the labels for the data points (0 or 1), separated by spaces:\")\n",
        "    y = list(map(int, input().split()))\n",
        "\n",
        "    return np.array(X), np.array(y), input_size\n"
      ],
      "metadata": {
        "id": "cYPXH04J3rMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_weights_bias(input_size):\n",
        "    # Get initial weights and bias from the user\n",
        "    weights = []\n",
        "    print(f\"Enter initial weights for {input_size} features, followed by the bias (e.g., '0.1 0.2 0.3'): \")\n",
        "    weights = list(map(float, input().split()))\n",
        "\n",
        "    if len(weights) != input_size + 1:\n",
        "        print(f\"Error: The number of weights and bias should be {input_size + 1}.\")\n",
        "        return None\n",
        "\n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "c_zj-TB23yWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(gate_name, perceptron, X):\n",
        "    print(f\"{gate_name} Gate:\")\n",
        "    print(f\"Weights: {perceptron.get_weights()[1:]}\")\n",
        "    print(f\"Bias: {perceptron.get_weights()[0]}\")\n",
        "    for xi in X:\n",
        "        print(f\"Input: {xi} => Prediction: {perceptron.predict(xi)}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "ixm1OgS9313r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"Perceptron Training for Boolean Gates\")\n",
        "    print(\"1. AND Gate\")\n",
        "    print(\"2. OR Gate\")\n",
        "    print(\"3. NOR Gate\")\n",
        "    print(\"4. NOT Gate\")\n",
        "    print(\"5. NAND Gate\")\n",
        "    print(\"6. XOR Gate\")\n",
        "\n",
        "    choice = input(\"Choose a gate to implement (1-6): \")\n",
        "\n",
        "    # Define gates as choices\n",
        "    gates = {\n",
        "        \"1\": \"AND\",\n",
        "        \"2\": \"OR\",\n",
        "        \"3\": \"NOR\",\n",
        "        \"4\": \"NOT\",\n",
        "        \"5\": \"NAND\",\n",
        "        \"6\": \"XOR\"\n",
        "    }\n",
        "\n",
        "    gate = gates.get(choice)\n",
        "    if not gate:\n",
        "        print(\"Invalid choice.\")\n",
        "        return\n",
        "\n",
        "    if gate == \"NOT\":\n",
        "        # For NOT gate, the input size is 1 and we only need one weight and one bias.\n",
        "        X, y, _ = get_user_input()\n",
        "        if X.shape[1] != 1:\n",
        "            print(\"Error: For the NOT gate, the number of features should be 1.\")\n",
        "            return\n",
        "        weights = get_user_weights_bias(1)\n",
        "        if weights is None:\n",
        "            return\n",
        "        perceptron = Perceptron(input_size=1, weights=weights)\n",
        "        perceptron.fit(X, y)\n",
        "        print_results(gate, perceptron, X)\n",
        "    else:\n",
        "        X, y, input_size = get_user_input()\n",
        "        if X.shape[0] != len(y):\n",
        "            print(\"Error: The number of data points and labels must match.\")\n",
        "            return\n",
        "\n",
        "        weights = get_user_weights_bias(input_size)\n",
        "        if weights is None:\n",
        "            return\n",
        "\n",
        "        perceptron = Perceptron(input_size=input_size, weights=weights)\n",
        "\n",
        "        # XOR gate requires a multi-layer perceptron\n",
        "        if gate == \"XOR\":\n",
        "            print(f\"{gate} Gate cannot be solved by a single perceptron.\")\n",
        "            print(\"For XOR, you would need a multi-layer perceptron or a different approach.\")\n",
        "            return\n",
        "\n",
        "        perceptron.fit(X, y)\n",
        "        print_results(gate, perceptron, X)\n"
      ],
      "metadata": {
        "id": "YtPGGCjt37IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGcTEfu24Cn7",
        "outputId": "012f46a8-55a9-4bf7-e8bd-5f222c9c7ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Training for Boolean Gates\n",
            "1. AND Gate\n",
            "2. OR Gate\n",
            "3. NOR Gate\n",
            "4. NOT Gate\n",
            "5. NAND Gate\n",
            "6. XOR Gate\n",
            "Choose a gate to implement (1-6): 6\n",
            "Define your dataset for training the perceptron.\n",
            "Enter the number of features (e.g., 2 for x1, x2): 2\n",
            "Enter the number of data points: 4\n",
            "Enter 4 data points, each with 2 features (e.g., '0 1' for 2 features):\n",
            "0 0\n",
            "1 0\n",
            "0 1\n",
            "1 1\n",
            "Enter the labels for the data points (0 or 1), separated by spaces:\n",
            "0 0 0 1\n",
            "Enter initial weights for 2 features, followed by the bias (e.g., '0.1 0.2 0.3'): \n",
            "0.1 0.2 0.3\n",
            "XOR Gate cannot be solved by a single perceptron.\n",
            "For XOR, you would need a multi-layer perceptron or a different approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linearly Separable Data Only: Can only solve problems where classes are separable by a single straight line. It fails with non-linearly separable data (e.g., XOR).\n",
        "\n",
        "Limited Expressive Power: Can only model linear decision boundaries and cannot capture complex patterns.\n",
        "\n",
        "Fixed Activation Function: Uses a binary step function, which is non-differentiable and limits learning capabilities.\n",
        "\n",
        "Learning Rate Sensitivity: Requires careful tuning of the learning rate to avoid slow convergence or instability.\n",
        "\n",
        "Overlapping Classes: Struggles with overlapping classes and noisy data, leading to potential misclassification.\n",
        "\n",
        "Training Convergence: Guarantees convergence only for linearly separable data, not for complex or non-linear cases."
      ],
      "metadata": {
        "id": "7QA-xrepx4F1"
      }
    }
  ]
}